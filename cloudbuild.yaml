# cloudbuild.yaml (v7 - Stream tar from container using docker exec | gsutil cp)

substitutions:
  # ... (substitutions remain the same) ...
  _SERVICE_NAME: web-app
  _DEPLOY_REGION: europe-west2
  _AR_HOSTNAME: europe-west2-docker.pkg.dev
  _AR_PROJECT_ID: apprenticewatch-55cb9
  _AR_REPOSITORY: cloud-run-source-deploy
  _PLATFORM: managed
  _IMAGE_NAME: ${_AR_HOSTNAME}/${_AR_PROJECT_ID}/${_AR_REPOSITORY}/${REPO_NAME}/${_SERVICE_NAME}
  _CACHE_BUCKET: gs://apprentice-watch-web-build-cache-g
  _CACHE_ARCHIVE_NODE: cache-node_modules.tar.gz
  _CACHE_ARCHIVE_NEXT: cache-next.tar.gz
  _NEXT_PUBLIC_SUPABASE_URL: https://swtrxonxzchgudehqdge.supabase.co
  _NEXT_PUBLIC_SUPABASE_ANON_KEY: 'your-anon-key'
  _NEXT_PUBLIC_BASE_URL: https://apprenticewatch.com
  _NEXT_PUBLIC_LOGODEV_KEY: 'your-logodev-key'
  _NEXT_PUBLIC_GEMINI_API_KEY: 'your-gemini-key'
  _NEXT_PUBLIC_MAPBOX_TOKEN: 'your-mapbox-token'
  _NEXT_PUBLIC_GA_TRACKING_ID: G-1GKBR9HKF3
  _REVALIDATION_SECRET_TOKEN: 'your-secret-token'

steps:
  # Step 0: Get package lock hash (Unchanged)
  - name: 'bash'
    id: 'Get package lock hash'
    script: |
      #!/usr/bin/env bash
      set -e
      LOCKFILE_HASH="no-lock-file"
      if [[ -f "package-lock.json" ]]; then
        LOCKFILE_HASH=$(sha1sum package-lock.json | awk '{ print $1 }')
        echo "Using package-lock.json hash: ${LOCKFILE_HASH}"
      elif [[ -f "yarn.lock" ]]; then
        LOCKFILE_HASH=$(sha1sum yarn.lock | awk '{ print $1 }')
        echo "Using yarn.lock hash: ${LOCKFILE_HASH}"
      else
        echo "Warning: No lock file found."
      fi
      echo "${LOCKFILE_HASH}" > /workspace/pkg_lock_sha.txt

  # Step 1: Download node_modules cache (Unchanged)
  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'Download node_modules cache'
    script: |
      #!/usr/bin/env bash
      set -e
      LOCK_HASH=$(cat /workspace/pkg_lock_sha.txt)
      # Create empty file marker if download is skipped or fails
      touch "/workspace/${_CACHE_ARCHIVE_NODE}.missing_or_failed"
      if [[ "${LOCK_HASH}" == "no-lock-file" ]]; then
        echo "Skipping node_modules cache download (no lock file)."
        exit 0
      fi
      CACHE_FILE_GCS_PATH="${_CACHE_BUCKET}/${LOCK_HASH}-${_CACHE_ARCHIVE_NODE}"
      LOCAL_CACHE_FILE="/workspace/${_CACHE_ARCHIVE_NODE}"
      echo "Attempting to download node_modules cache: ${CACHE_FILE_GCS_PATH}"
      if (gsutil -q cp "${CACHE_FILE_GCS_PATH}" "${LOCAL_CACHE_FILE}"); then
        # Remove marker file on success
        rm -f "/workspace/${_CACHE_ARCHIVE_NODE}.missing_or_failed"
      else
        echo "Node modules cache not found or download failed for hash ${LOCK_HASH}."
      fi

  # Step 2: Extract node_modules cache (Unchanged)
  - name: 'ubuntu'
    id: 'Extract node_modules cache'
    script: |
       #!/usr/bin/env bash
       set -e
       LOCAL_CACHE_FILE="/workspace/${_CACHE_ARCHIVE_NODE}"
       if [[ -f "${LOCAL_CACHE_FILE}" ]]; then
         echo "Extracting node_modules cache into /workspace/node_modules..."
         mkdir -p /workspace/node_modules
         tar -xzmf "${LOCAL_CACHE_FILE}" -C /workspace || echo "Warning: Failed to extract node_modules cache, continuing..."
         rm "${LOCAL_CACHE_FILE}"
         echo "node_modules cache extracted."
       else
         echo "No node_modules cache archive found to extract."
       fi
    waitFor: ['Download node_modules cache']

  # Step 3: Download .next/cache (Unchanged)
  - name: 'gcr.io/cloud-builders/gsutil'
    id: 'Download .next cache'
    script: |
      #!/usr/bin/env bash
      set -e
      touch "/workspace/${_CACHE_ARCHIVE_NEXT}.missing_or_failed"
      CACHE_FILE_GCS_PATH="${_CACHE_BUCKET}/latest-${_CACHE_ARCHIVE_NEXT}"
      LOCAL_CACHE_FILE="/workspace/${_CACHE_ARCHIVE_NEXT}"
      echo "Attempting to download .next cache: ${CACHE_FILE_GCS_PATH}"
      if (gsutil -q cp "${CACHE_FILE_GCS_PATH}" "${LOCAL_CACHE_FILE}"); then
        rm -f "/workspace/${_CACHE_ARCHIVE_NEXT}.missing_or_failed"
      else
        echo ".next cache not found or download failed."
      fi

  # Step 4: Extract .next cache (Unchanged)
  - name: 'ubuntu'
    id: 'Extract .next cache'
    script: |
       #!/usr/bin/env bash
       set -e
       LOCAL_CACHE_FILE="/workspace/${_CACHE_ARCHIVE_NEXT}"
       if [[ -f "${LOCAL_CACHE_FILE}" ]]; then
         echo "Extracting .next cache into /workspace/.next/cache..."
         mkdir -p /workspace/.next/cache
         tar -xzmf "${LOCAL_CACHE_FILE}" --strip-components=1 -C /workspace/.next/cache || echo "Warning: Failed to extract .next cache, continuing..."
         rm "${LOCAL_CACHE_FILE}"
         echo ".next cache extracted."
       else
         echo "No .next cache archive found to extract."
       fi
    waitFor: ['Extract node_modules cache', 'Download .next cache']

  # Step 5: Pull latest image for Docker cache (Unchanged)
  - name: 'gcr.io/cloud-builders/docker'
    id: 'Pull latest image for cache'
    entrypoint: 'bash'
    args: ['-c', 'docker pull ${_IMAGE_NAME}:latest || exit 0']
    waitFor: ['-']

  # Step 6: Build the Docker image (Unchanged)
  - name: 'gcr.io/cloud-builders/docker'
    id: 'Build'
    args: [
            'build',
            '--tag', '${_IMAGE_NAME}:${COMMIT_SHA}',
            '--tag', '${_IMAGE_NAME}:latest',
            '--cache-from', '${_IMAGE_NAME}:latest',
            '--build-arg', 'NEXT_PUBLIC_SUPABASE_URL=${_NEXT_PUBLIC_SUPABASE_URL}',
            '--build-arg', 'NEXT_PUBLIC_SUPABASE_ANON_KEY=${_NEXT_PUBLIC_SUPABASE_ANON_KEY}',
            '--build-arg', 'NEXT_PUBLIC_BASE_URL=${_NEXT_PUBLIC_BASE_URL}',
            '--build-arg', 'NEXT_PUBLIC_LOGODEV_KEY=${_NEXT_PUBLIC_LOGODEV_KEY}',
            '--build-arg', 'NEXT_PUBLIC_GEMINI_API_KEY=${_NEXT_PUBLIC_GEMINI_API_KEY}',
            '--build-arg', 'NEXT_PUBLIC_MAPBOX_TOKEN=${_NEXT_PUBLIC_MAPBOX_TOKEN}',
            '--build-arg', 'NEXT_PUBLIC_GA_TRACKING_ID=${_NEXT_PUBLIC_GA_TRACKING_ID}',
            '.',
        ]
    waitFor: ['Extract .next cache', 'Pull latest image for cache']

  # --- STEPS REPLACED/MODIFIED ---

  # Step 7: Create dummy container from build image (Unchanged)
  - name: 'gcr.io/cloud-builders/docker'
    id: 'Create dummy container'
    args: ['create', '--name', 'extractor', '${_IMAGE_NAME}:${COMMIT_SHA}']
    waitFor: ['Build']

  # Step 8: Archive and Upload node_modules directly from container
  # Use gcloud image as it has docker, gsutil, and bash/tar
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk:slim'
    id: 'Archive and Upload node_modules'
    script: |
      #!/usr/bin/env bash
      set -e
      LOCK_HASH=$(cat /workspace/pkg_lock_sha.txt)
      # Only upload if cache wasn't downloaded/extracted OR if lock hash is "no-lock-file" (meaning we must create it)
      # Check for the marker file created in step 1
      if [[ -f "/workspace/${_CACHE_ARCHIVE_NODE}.missing_or_failed" ]] || [[ "${LOCK_HASH}" == "no-lock-file" ]]; then
        if [[ "${LOCK_HASH}" == "no-lock-file" ]]; then
          echo "Warning: No lock file. node_modules cache will be uploaded without specific hash."
          # Optionally, you could skip upload here if no lock file means no reliable cache
          # exit 0
          CACHE_FILE_GCS_PATH="${_CACHE_BUCKET}/no-lock-file-${_CACHE_ARCHIVE_NODE}" # Or handle differently
        else
          CACHE_FILE_GCS_PATH="${_CACHE_BUCKET}/${LOCK_HASH}-${_CACHE_ARCHIVE_NODE}"
        fi

        echo "Archiving node_modules from container and uploading to ${CACHE_FILE_GCS_PATH}..."
        # Execute tar inside the container, pipe stdout to gsutil reading stdin (-)
        # Use -C inside container to get relative paths in archive
        docker exec extractor tar --ignore-failed-read -czf - -C /app node_modules | \
          gsutil -q -h "Cache-Control:private, max-age=0, no-transform" cp - "${CACHE_FILE_GCS_PATH}"

        echo "node_modules archive uploaded."
        # Clean up marker file if it exists
        rm -f "/workspace/${_CACHE_ARCHIVE_NODE}.missing_or_failed"
      else
        echo "Skipping node_modules archive and upload (cache likely extracted from GCS)."
      fi
    waitFor: ['Create dummy container', 'Extract node_modules cache'] # Wait for container and extraction attempt

  # Step 9: Archive and Upload .next/cache directly from container
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk:slim'
    id: 'Archive and Upload .next cache'
    script: |
      #!/usr/bin/env bash
      set -e
      # Only upload if cache wasn't downloaded/extracted
      # Check for the marker file created in step 3
      if [[ -f "/workspace/${_CACHE_ARCHIVE_NEXT}.missing_or_failed" ]]; then
        CACHE_FILE_GCS_PATH="${_CACHE_BUCKET}/latest-${_CACHE_ARCHIVE_NEXT}"
        echo "Archiving .next/cache from container and uploading to ${CACHE_FILE_GCS_PATH}..."

        # Execute tar inside the container for .next/cache, pipe stdout to gsutil reading stdin (-)
        # Use -C inside container to archive relative path 'cache' from within '.next'
        docker exec extractor tar --ignore-failed-read -czf - -C /app/.next cache | \
          gsutil -q -h "Cache-Control:private, max-age=0, no-transform" cp - "${CACHE_FILE_GCS_PATH}"

        echo ".next/cache archive uploaded."
        # Clean up marker file if it exists
        rm -f "/workspace/${_CACHE_ARCHIVE_NEXT}.missing_or_failed"
      else
        echo "Skipping .next/cache archive and upload (cache likely extracted from GCS)."
      fi
    waitFor: ['Create dummy container', 'Extract .next cache'] # Wait for container and extraction attempt

  # Step 10: Remove the dummy container (Unchanged)
  - name: 'gcr.io/cloud-builders/docker'
    id: 'Remove dummy container'
    args: ['rm', 'extractor']
    # Wait for both archive/upload steps to finish using the container
    waitFor: ['Archive and Upload node_modules', 'Archive and Upload .next cache']

  # Step 11 (was 15): Push the container image tags
  - name: 'gcr.io/cloud-builders/docker'
    id: 'Push SHA tag'
    args: ['push', '${_IMAGE_NAME}:${COMMIT_SHA}']
    # Wait for uploads to finish (redundant now?) and container removal
    waitFor: ['Remove dummy container'] # Simplified wait, uploads happen before removal

  - name: 'gcr.io/cloud-builders/docker'
    id: 'Push latest tag'
    args: ['push', '${_IMAGE_NAME}:latest']
    waitFor: ['Push SHA tag']

  # Step 12 (was 16): Deploy to Cloud Run (Unchanged)
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk:slim'
    id: 'Deploy'
    entrypoint: gcloud
    args:
      - run
      - services
      - update
      - ${_SERVICE_NAME}
      - '--platform=managed'
      - '--image=${_IMAGE_NAME}:${COMMIT_SHA}'
      - '--labels=managed-by=gcp-cloud-build-deploy-cloud-run,commit-sha=${COMMIT_SHA},gcb-build-id=${BUILD_ID},gcb-trigger-id=${_TRIGGER_ID}'
      - '--region=${_DEPLOY_REGION}'
      - '--quiet'
      - '--set-env-vars=NEXT_PUBLIC_SUPABASE_URL=${_NEXT_PUBLIC_SUPABASE_URL}'
      - '--set-env-vars=NEXT_PUBLIC_SUPABASE_ANON_KEY=${_NEXT_PUBLIC_SUPABASE_ANON_KEY}'
      - '--set-env-vars=NEXT_PUBLIC_BASE_URL=${_NEXT_PUBLIC_BASE_URL}'
      - '--set-env-vars=NEXT_PUBLIC_LOGODEV_KEY=${_NEXT_PUBLIC_LOGODEV_KEY}'
      - '--set-env-vars=NEXT_PUBLIC_GEMINI_API_KEY=${_NEXT_PUBLIC_GEMINI_API_KEY}'
      - '--set-env-vars=NEXT_PUBLIC_MAPBOX_TOKEN=${_NEXT_PUBLIC_MAPBOX_TOKEN}'
      - '--set-env-vars=NEXT_PUBLIC_GA_TRACKING_ID=${_NEXT_PUBLIC_GA_TRACKING_ID}'
      - '--set-env-vars=REVALIDATION_SECRET_TOKEN=${_REVALIDATION_SECRET_TOKEN}'
    waitFor: ['Push latest tag']

# List images built
images:
  - '${_IMAGE_NAME}:${COMMIT_SHA}'
  - '${_IMAGE_NAME}:latest'

options:
  # machineType: 'E2_HIGHCPU_8'
  substitutionOption: ALLOW_LOOSE
  logging: CLOUD_LOGGING_ONLY

tags:
  - gcp-cloud-build-deploy-cloud-run
  - gcp-cloud-build-deploy-cloud-run-managed
  - web-app